{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy-Based Methods\n",
    "Whereas *Value-Based Methods* like Deep Q-Learning are obtaining an optimal policy $\\pi_*$ by trying to estimate the optimal action-value function, *Policy-Based Methods* directly learn the optimal policy.  \n",
    "Besides this simplification another advantage of a Policy-Based Method is the fact that it is able to handle either stochastic or continuous actions.  \n",
    "On the one hand Policy-Based Methods are using the *Monte Carlo* (MC) approach for the estimate of expected return:\n",
    "\n",
    "$ G_t = R_{t+1} + R_{t+2} + ... + R_T$, if the discount factor $\\gamma=1$\n",
    "\n",
    "As $G_t$ is estimated with the full trajectory this yields to a high *variance*, but to a low *bias*.  \n",
    "On the other hand Value-Based Methods are using the *Temporal Difference* (TD) approach to estimate the return:\n",
    "\n",
    "$ G_t = R_{t+1} + G_{t+1}$ , if $\\gamma=1$\n",
    "\n",
    "Here $G_{t+1}$ is the estimated total return an agent will obtain in the next state. As the estimate of $G_t$ is always depending on the estimate of the next state, the variance of these estimates is low but biased.  \n",
    "The pros of both methods can be combined in one single algorithm namely the Actor-Critic Method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actor-Critic Methods\n",
    "In Actor-Critic Methods one uses two function approximators (usually neural networks) to learn a policy (Actor) and a value function (Critic). The process looks as follows:  \n",
    "\n",
    "1) Observe state $s$ from environment and feed into the Actor.  \n",
    "2) The output are action probabilities $\\pi(a|s;\\theta_\\pi)$. Select one action stochastically and feed back to the environment.  \n",
    "3) Observe next state $s'$ and reward $r$.  \n",
    "4) Use the tuple $(s, a, r, s')$ for the TD estimate $y=r + \\gamma V(s'; \\theta_v)$  \n",
    "5) Train the Critic by minimizing the loss $L=(y - V(s;\\theta_v)^2$.  \n",
    "6) Calculate the advantage $A(s,a) = r + \\gamma V(s'; \\theta_v) - V(s; \\theta_v)$.  \n",
    "7) Train the Actor using the advantage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Deterministic Policy Gradient\n",
    "The following section refers to [Lillicrap et al., 2016].  \n",
    "Deep Deterministic Policy Gradient (DDPG) combines the Actor-Critic approach with Deep Q-Learning. The actor function $\\mu(s;\\theta_\\mu)$ gives the current policy. It maps states to continuous deterministic actions. The critic $Q(s,a;\\theta_q)$ on the other hand is used to calculate action values and is learned using the Bellman equation. DDPG is also using a *replay buffer* and *target networks* which already helped to improve performance for Deep Q-Learning. In a finite replay buffer tuples of $(s, a, r, s')$ are stored and then batches are sampled from this buffer to apply for network updates. This tackles the issue of correlated tuples arrised from sequentially exploring the environment. Target networks are used to decouple the TD target from the current action value when performing neutwork updates. The target network is a copy of the Actor and Critic Network which are used to calculated the target. One approach is to update the weights of the target networks $\\theta'$ with the weights $\\theta$ of the Actor and Critic network periodically. An other approach is to perform *soft updates*:\n",
    "\n",
    "$ \\theta' \\leftarrow \\tau \\theta + (1-\\tau)\\theta'$ with $\\tau \\ll 1$\n",
    "\n",
    "In order to scale features *batch normalization* is being applied. This normalizes each dimension across the samples of the minibatch. An other important issue is handling exploration. By adding a noise process $N$ an exploration policy $\\mu'$ is constructed:\n",
    "\n",
    "$\\mu'(s_t) = \\mu(s_t;\\theta_{\\mu,t})+N$\n",
    "\n",
    "The DDPG process looks as follows:  \n",
    "1) Observe state $s$ from environment and feed to Actor.  \n",
    "2) Select action $a = \\mu(s;\\theta_\\mu) + N$ and feed back to environment.  \n",
    "3) Observe next state $s'$ and reward $r$.  \n",
    "4) Store transition $(s, a, r, s')$ in replay buffer and sample random minibatch of $n$ tuples. Calculate the TD estimate \n",
    "$y = r + \\gamma Q'(s', \\mu'(s';\\theta_\\mu');\\theta_q')$  \n",
    "5) Train the Critic by minimizing the loss \n",
    "$L=\\mathbb{E} \\big[\\big(y - Q(s,a;\\theta_q)\\big)^2\\big]$  \n",
    "6) Train Actor with policy gradient \n",
    "$\\mathbb{E} \\big[\\nabla_{\\theta_\\mu} Q(s,a;\\theta_q) | s=s_t, a=\\mu(s_t;\\theta_\\mu) \\big] = \\mathbb{E} \\big[\\nabla_a Q(s,a;\\theta_q)|s=s_t,a=\\mu(s_t) \\nabla_{\\theta_\\mu} \\mu(s;\\theta_\\mu)|s=s_t\\big] $  \n",
    "7) Update both target networks using soft update\n",
    "\n",
    "As one see, this is an off-policy algorithm because the policy which is evaluated uses action $a=\\mu'(s';\\theta_\\mu')$. This is different from the policy which selects action $a = \\mu(s;\\theta_\\mu) + N$. An other interesting aspect is that the Critic network has only one output node, which is the action value given the state and the action: $Q(s,a;\\theta_q)$ This is different to Deep Q-Learning where the Q-Network is mapping values to every possible (discrete) action node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install /python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage.filters import gaussian_filter1d\n",
    "from collections import deque\n",
    "\n",
    "from agent import Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 2000\n",
    "BUFFER_SIZE = int(1e7)\n",
    "BATCH_SIZE = 256\n",
    "GAMMA = .99\n",
    "TAU = 1e-3\n",
    "LEARNING_RATE_ACTOR = 1e-4\n",
    "LEARNING_RATE_CRITIC = 1e-3\n",
    "WEIGHT_DECAY = 0.0\n",
    "SEED = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_env(unity_file):\n",
    "    # Initialize the environment\n",
    "    env = UnityEnvironment(file_name=unity_file, worker_id=2)\n",
    "\n",
    "    # Get default brain\n",
    "    brain_name = env.brain_names[0]\n",
    "    brain = env.brains[brain_name]\n",
    "\n",
    "    # Get state and action spaces\n",
    "    env_info = env.reset(train_mode=True)[brain_name]\n",
    "    state_size = env_info.vector_observations.shape[1]\n",
    "    action_size = brain.vector_action_space_size\n",
    "    n_agents = len(env_info.agents)\n",
    "    \n",
    "    print('State size: ', state_size)\n",
    "    print('Action size: ', action_size)\n",
    "    print('Number of agents: ', n_agents)\n",
    "    \n",
    "    return env, brain_name, state_size, action_size, n_agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State size:  33\n",
      "Action size:  4\n",
      "Number of agents:  20\n"
     ]
    }
   ],
   "source": [
    "env, brain_name, state_size, action_size, n_agents = \\\n",
    "        initialize_env('/data/Reacher_Linux_NoVis/Reacher.x86')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize agent\n",
    "agent = Agent(state_size, action_size,\n",
    "              n_agents, buffer_size=BUFFER_SIZE, \n",
    "              batch_size=BATCH_SIZE, gamma=GAMMA, tau=TAU,\n",
    "              lr_a=LEARNING_RATE_ACTOR, lr_c=LEARNING_RATE_CRITIC,\n",
    "              weight_decay=WEIGHT_DECAY, random_seed=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ddpg(env, brain_name,\n",
    "         agent, n_agents,\n",
    "         n_episodes=2000, t_max=3000):\n",
    "    \"\"\"Deep Determinitic Policy Gradient.\n",
    "\n",
    "    Params\n",
    "    ======\n",
    "        env: unity environment object\n",
    "        brain_name (string): brain name of initialized environment\n",
    "        agent: initialized agent object\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        t_max (int): maximum timesteps in episode\n",
    "    \"\"\"\n",
    "    \n",
    "    scores = []\n",
    "    scores_window = deque(maxlen=100)\n",
    "    for e in range(1, n_episodes+1):\n",
    "        env_info = env.reset(train_mode=True)[brain_name]\n",
    "        state = env_info.vector_observations\n",
    "        agent.reset()\n",
    "        score = np.zeros(n_agents)\n",
    "        for _ in range(1, t_max):\n",
    "            action = agent.act(state)\n",
    "            env_info = env.step(action)[brain_name]\n",
    "            next_state = env_info.vector_observations\n",
    "            reward = env_info.rewards\n",
    "            done = env_info.local_done\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            score += reward\n",
    "            state = next_state\n",
    "            if np.any(done):\n",
    "                break\n",
    "\n",
    "        # Relative score\n",
    "        avg_score = np.mean(score)\n",
    "        scores_window.append(avg_score)\n",
    "        scores.append(avg_score)\n",
    "\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(e, np.mean(scores_window)), end=\"\")\n",
    "        if e % 10 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(e, np.mean(scores_window)))\n",
    "        if np.mean(scores_window)>=30.0:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(e-100, np.mean(scores_window)))\n",
    "            torch.save(agent.actor_local.state_dict(), 'checkpoint_actor.pth')\n",
    "            torch.save(agent.critic_local.state_dict(), 'checkpoint_critic.pth')\n",
    "            break\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scores(scores_dict):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    for key, scores in scores_dict.items():\n",
    "        scores_smoothed = gaussian_filter1d(scores, sigma=5)\n",
    "        plt.plot(np.arange(len(scores)), scores_smoothed, label=key)\n",
    "    plt.ylabel('smoothed Score')\n",
    "    plt.xlabel('Episode #')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 10\tAverage Score: 0.87\n",
      "Episode 20\tAverage Score: 1.10\n",
      "Episode 30\tAverage Score: 1.59\n",
      "Episode 40\tAverage Score: 2.33\n",
      "Episode 50\tAverage Score: 3.20\n",
      "Episode 60\tAverage Score: 4.54\n",
      "Episode 70\tAverage Score: 5.91\n",
      "Episode 80\tAverage Score: 7.35\n",
      "Episode 90\tAverage Score: 8.70\n",
      "Episode 100\tAverage Score: 9.90\n",
      "Episode 110\tAverage Score: 11.97\n",
      "Episode 120\tAverage Score: 14.00\n",
      "Episode 130\tAverage Score: 15.99\n",
      "Episode 140\tAverage Score: 17.88\n",
      "Episode 150\tAverage Score: 19.72\n",
      "Episode 160\tAverage Score: 21.15\n",
      "Episode 170\tAverage Score: 22.62\n",
      "Episode 180\tAverage Score: 23.91\n",
      "Episode 190\tAverage Score: 25.07\n",
      "Episode 200\tAverage Score: 26.19\n",
      "Episode 210\tAverage Score: 27.21\n",
      "Episode 220\tAverage Score: 28.31\n",
      "Episode 230\tAverage Score: 29.39\n",
      "Episode 236\tAverage Score: 30.04\n",
      "Environment solved in 236 episodes!\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f961c065fd0>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train agent\n",
    "scores = ddpg(env, brain_name, agent, n_agents, n_episodes=N)\n",
    "\n",
    "plot_scores({'DDPG': scores})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further potentials to improvement the RL agent\n",
    "The algorithm took quite long to solve the environment. One potential option would be to reduce update frequency in order to achieve a more stable learning.  \n",
    "Other algorithms that could solve this environment are  PPO, A3C and D4PG."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "- Lillicrap, T., Hunt, J., Pritzel, A., Heess, N., Erez, T., Tassa, Y., Silver, D., and Wiestra, D., Continuous Control with Deep Reinforcement Learning, arXiv:1509.02971v5 [cs.LG] 29 Feb 2016"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
