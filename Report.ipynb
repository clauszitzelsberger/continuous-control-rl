{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy-Based Methods\n",
    "Whereas *Value-Based Methods* like Deep Q-Learning are obtaining an optimal policy $\\pi_*$ by trying to estimate the optimal action-value function, *Policy-Based Methods* directly learn the optimal policy.  \n",
    "Besides this simplification another advantage of a Policy-Based Method is the fact that it is able to handle either stochastic or continuous actions.  \n",
    "On the one hand Policy-Based Methods are using the *Monte Carlo* (MC) approach for the estimate of expected return:\n",
    "\n",
    "$ G_t = R_{t+1} + R_{t+2} + ... + R_T$, if the discount factor $\\gamma=1$\n",
    "\n",
    "As $G_t$ is estimated with the full trajectory this yields to a high *variance*, but to a low *bias*.  \n",
    "On the other hand Value-Based Methods are using the *Temporal Difference* (TD) approach to estimate the return:\n",
    "\n",
    "$ G_t = R_{t+1} + G_{t+1}$ , if $\\gamma=1$\n",
    "\n",
    "Here $G_{t+1}$ is the estimated total return an agent will obtain in the next state. As the estimate of $G_t$ is always depending on the estimate of the next state, the variance of these estimates is low but biased.  \n",
    "The pros of both methods can be combined in one single algorithm namely the Actor-Critic Method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actor-Critic Methods\n",
    "In Actor-Critic Methods one uses two function approximators (usually neural networks) to learn a policy (Actor) and a value function (Critic). The process looks as follows:  \n",
    "\n",
    "1) Observe state $s$ from environment and feed into the Actor.  \n",
    "2) The output are action probabilities $\\pi(a|s;\\theta_\\pi)$. Select one action a stochastically and fede back to the environment.  \n",
    "3) Observe next state $s'$ and reward $r$.  \n",
    "4) Use the tuple $(s, a, r, s')$ for the TD estimate $r + \\gamma V(s'; \\theta_v)$ to train the Critic.  \n",
    "5) Calculate the advantage $A(s,a) = r + \\gamma V(s'; \\theta_v) - V(s; \\theta_v)$.  \n",
    "6) Train the Actor using the advantage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Deterministic Policy Gradient\n",
    "Deep Deterministic Policy Gradient (DDPG) combines the actor critic approach with Deep Q-Learning. The actor function $\\mu(s|\\theta_\\mu)$ gives the current policy. It maps states to continuous actions. The critic $Q(s,a)$ on the other hand "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "- Lillicrap, T., Hunt, J., Pritzel, A., Heess, N., Erez, T., Tassa, Y., Silver, D., and Wiestra, D., Continuous Control with Deep Reinforcement Learning, arXiv:1509.02971v5 [cs.LG] 29 Feb 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
